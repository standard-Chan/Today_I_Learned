# 1. 벡터 DB의 필요성

벡터 DB의 필요성을 이해하기 전에 검색이 어떻게 발전해왔는지를 이해할 필요가 있다.

## 검색

초기 검색은 단어 자체가 포함되어 있는지를 기반으로 작동된다.

만약, "귀여운 고양이 사진"을 검색하면, 시스템이 "귀여운", "고양이", "사진"이라는 단어를 포함하는 데이터들을 찾아오는 방식이다.

하지만 위와같은 방식은 다음과 같은 문제점이 있다.

- "스마트폰 추천"을 검색하면 "아이폰 추천", "갤럭시 폰 추천"은 나오지 않는다.
- 맥락, 의미를 고려하지 않는다.

이는 근본적으로 RDB(관계형 데이터베이스)의 한계 때문이다.

## RDB의 한계

관계형 데이터베이스는 정형화된 데이터(ex 식당{이름, 위치, 메뉴})를 처리하는데는 좋았지만, 이미지, 영상, 자연어와 같이 의미가 흐릿하고 맥락이 중요한 비정형 데이터를 처리하는데에는 취약하다.

왜냐하면 검색은 데이터가 해당 단어를 포함하는지(`LIKE '%단어%'`) 여부만 알아낼 수 있기 때문이다.

## 벡터 DB

여러 자연어 처리, 생성형 AI 등이 발전함에 따라서 여러 데이터(단어, 이미지, 영상)들을 `벡터`로 변환할 수 있게 되었다.

이를테면 아래와 같이 변경이 가능하다.
| 문장 | 임베딩 벡터 |
| ----------- | ------------------------- |
| "귀여운 고양이" | \[0.42, -0.13, 0.55, ...] |
| "사랑스러운 고양이" | \[0.41, -0.12, 0.56, ...] |

변환한 두 문장의 임베딩 벡터의 유사도를 측정하면 위 두 문장이 유사하다는 것을 알아낼 수 있다.

이러한 방식으로 문장사이의 유사도를 측정할 수 있는데, 문제가 있다.

> 수십억 개의 벡터 중에서 유사한 것을 어떻게 빠르게 찾을 수 있을까?

위 문제를 해결하기 위해서 `벡터 데이터베이스`가 등장했다. 수많은 벡터 데이터의 유사도 검색을 빠르게 하기 위해서 만들어진 DB가 벡터 데이터베이스이다. 다음은 대표적인 기능들이다.

- 수많은 벡터 중 가장 가까운 벡터들을 빠르게 찾는다.
- 텍스트, 이미지, 음성 등 다양한 비정형 데이터의 의미 기반 검색이 가능하다.
- GPT, CLIP, BERT 등 임베딩 모델과 결합해서, 진짜 ‘이해하는 검색’을 제공한다.

## 왜 벡터 DB를 배워야하는가?

어떤 서비스를 만들다보면 다음과 같은 의문이 들때가 있다.

> "XXX과 비슷한 걸 찾고 싶은데, 어떻게 구현하지?"

단순하게 단어를 검색 것을 넘어서 의미단위로 연결하고 이해하는 서비스를 만들고 싶다면 `벡터 DB`, `임베딩`, `유사도 검색`을 사용해야한다.

---

# 2. 벡터 DB 처리 흐름 이해

검색 요청을 보냈다고 했을 때, 실제로 "귀여운 고양이"를 검색했다고 했을 때, 벡터 DB 내부에서 어떤 일이 발생하는지를 이해해보자.

### 1) 검색요청 - 벡터 저장

#### 임베딩 -> 벡터 저장

우리가 입력한 "귀여운 고양이"가 `임베딩 모델`을 통해서 `고차원 벡터`로 변환된다.
예시: "귀여운 고양이" => [0.42, -0.13, 0.55, ..., 0.12] (이때 차원은 128 ~ 1600 차원 정도)

그리고 변환한 벡터 데이터가 ID, 메타데이터와 함께 저장된다.

```
벡터: [0.42, -0.13, 0.55, ..., 0.12]
ID: doc123
Metadata: {title: “고양이 사진”, tags: ["귀여움", "반려동물"]}
```

### 2) 벡터 유사도 검색

변환한 벡터 데이터의 유사도를 검색한다. 이때 저장 방식과 사용 목적에 따라 유사도를 검색하는 다양한 방식이 있다.

**`1. 코사인 유사도`**

- 두 벡터 사이의 각도가 얼마나 유사한지로 판단한다.

**`2. L2 거리`**

- 두 벡터 사이의 직선 거리로 판단

**`3. Inner Product`**

- 코사인 유사도와 비슷하게 방향으로 판단하지만, 벡터의 크기가 추가된다.

### 3) 인덱싱

벡터 유사도를 비교하기 위해서는 데이터를 직접 읽어야한다. 하지만 데이터의 수가 굉장히 많기 때문에 모든 데이터를 읽기에는 시간이 너무 오래걸리는데, 이를 해결하기 위해서 `ANN(Approximate Nearest Neighbor)` 알고리즘을 사용한다.

ANN 알고리즘에는 여러 종류가 있다.
**`1. HNSW(Hierarchical Navigable Small World)`**

- 벡터를 노드로 보고, 서로 가까운 노드끼리 연결해서 그래프를 만든다.
- 계층 구조를 만들어서 먼거리 -> 가까운 거리로 좁혀나간다.
- 먼거리는 대충 훑어보고, 가까이 다가갈수록 정교하게 확인한다.
- 속도: 빠름
- 정확도: 높음

**`2. IVF (Inverted File Index)`**

- 벡터를 여러 클러스터(centroid)로 나눈다. K개의 클러스터로 나눈다(K-Means)
- 검색 벡터와 가장 가까운 클러스터를 찾는다.
- 해당 클러스터 안에 있는 벡터들만 유사도를 계산한다.
- 속도: 빠름
- 정확도: 중간 이상

**`기타`**

- FAISS (Facebook AI Search): HNSW, IVF, PQ 등 다양한 인덱싱 지원
- ScaNN (Google): L2 최적화 + 학습 기반 인덱싱
- Annoy (Spotify): 트리 기반 검색 (코사인 거리)

### 정리

```text
[사용자 입력]
  ↓
[임베딩 모델로 벡터화]
  ↓
[벡터 DB에 유사 벡터 검색 요청]
  ↓
[HNSW/IVF 등의 인덱스에서 후보 벡터 추출]
  ↓
[후보 벡터와 정확한 유사도 계산]
  ↓
[Top-K 유사한 결과 추출]
  ↓
[해당 벡터에 연결된 원본 콘텐츠(Metadata, Doc 등) 반환]

```

---

# 3. 임베딩 및 벡터화

## 어떤 모델이 어떤 기준으로 벡터를 생성하는가?

### “모델”이란?

| 분류                   | 대표 모델                         | 설명                        |
| ---------------------- | --------------------------------- | --------------------------- |
| **텍스트 임베딩 모델** | BERT, SBERT, OpenAI Embedding, E5 | 문장을 벡터로 바꾸는 모델   |
| **이미지 임베딩 모델** | CLIP, DINO, ResNet                | 이미지를 벡터로             |
| **멀티모달 임베딩**    | CLIP, Flamingo, GILA              | 텍스트+이미지를 함께 임베딩 |

위 모델의 대부분이 `Transformer` 계열이고 그 구조 안에서 단어, 문장, 의미 사이의 관계를 학습한 모델을 바탕으로 벡터를 생성한다.

### 벡터는 어떻게 만들어질까?

벡터는 다음과 같이 생성된다.

```
[“귀여운 고양이 사진”]
   ↓  토크나이징 (단어 → 숫자 ID)
[101, 3782, 1234, 572]
   ↓  임베딩 계층 + Attention
   ↓  마지막 레이어의 출력값
[0.42, -0.13, 0.55, ..., 0.12]  ← 이게 최종 벡터
```

해당 벡터는 **의미 공간**에서 비슷한 문장들과 가깝게 위치하도록 훈련된 것이다.

의미공간에서 훈련됐다는 말이 무슨말일까? **이는 의미 공간이 어떻게 학습되는가?**와 비슷한 말인데, 핵심은 다음과 같다.

> "문장을 숫자로 바꿨을 대, 어떻게 비슷한 것끼리 가깝게 위치시킬 수 있을까?"

이를 위해 BERT 혹은 SBERT 모델이 사용된다.
| 항목 | BERT | SBERT |
| --------- | ------------ | ------------------- |
| 학습 목적 | 문장 내부 단어 예측 | 문장 간 유사도 반영 |
| 구조 | 단일 입력 / MLM | 문장쌍 입력 / Siamese 구조 |
| 벡터 의미 | 문맥 정보 포함된 요약 | **유사도 기반 위치 정보** |
| 유사 문장 임베딩 | 성능 낮음 | 성능 매우 우수 |

#### `BERT`

문장 내 단어의 관계 학습한다. 이 과정에서 MLM(Masked Language Modeling) 훈련 방식을 사용한다. 훈련 방식은 아래와 같다.

예: "나는 []를 마셨다."
정답 : 음료수

이걸 맞추기 위해서 BERT는 문장 내의 다른 단어들과의 관계를 학습하게 된다. 즉 정답 "음료수"를 '나는' 과 '마셨다.'와 연결시키는 것이다.

하지만 `단점`이 존재한다.
`[ ]` 내부의 단어와 그 주변 단어 사이의 관계는 잘 학습할 수 있지만, 그렇지 않은 문장들을 학습할 수는 없다.

즉, "나는 음료수를 마셨다"라는 문장와 "나는 음료수를 좋아해" 라는 문장이 가까워지도록 훈련시킬 수는 없다.

#### `SBERT (Sentence-BERT)`

문장 A와 B를 각각 BERT로 임베딩하여 두 문장 벡터의 유사도를 계산한다. 그리고 이를 실제 유사도 점수와 가까워지도록 학습한다.

예를 들어서 다음 학습 데이터가 주어진다고 해보자.
| 문장 A | 문장 B | 유사도 레이블 (0\~5) |
| ----------- | ----------- | -------------- |
| “나는 음료수를 마셨다” | “나는 음료수를 좋아해” | 5 |
| “나는 고양이를 좋아해” | “나는 음료수를 좋아해” | 2 |
| “서울에 갔어” | '나는 음료수를 좋아해" | 0 |

`유사도 레이블` 점수에 따라서 문장 A와 문장 B의 관계를 학습하고 이 문장들을 벡터 공간에 얼마나 가까이 놓을지를 학습한다.

위 방식을 이용하면 `BERT`에서 발생한 문제를 해결할 수 있다.

### 같은 문장을 매번 같은 벡터로 만들 수 있을까?

답부터 말하자면 "보통은 같은 벡터가 나온다."
대부분의 임베딩 모델이 결정론적이기 때문에 동일한 입력에 대해서 항상 동일한 출력을 반환한다.

### 벡터는 어떤 형태로 저장되는가?

벡터 DB에 데이터를 넣는 절차를 통해 이해해보자.

1. 입력 문장을 벡터화 (임베딩)

- 문장 : "고양이는 귀엽다"
- 임베딩 모델을 통해서 고차원 벡터로 변환한다.

2. 벡터 + 메타데이터 + ID 묶음 생성

- id(식별자), vector(벡터), metadata(문장 설명, 태그 등) 을 추가하여 벡터 데이터를 생성한다.
- 예시
  ```json
  {
    "id": "cat_001",
    "vector": [0.42, -0.13, 0.55, ..., 0.12],
    "metadata": {
      "text": "고양이는 귀엽다",
      "tags": ["고양이", "동물", "귀여움"]
    }
  }
  ```

3. 벡터 DB에 저장 요청 (insert/upsert)

- 벡터를 저장
- 인덱스에 반영

---

# 4. 유사도 계산

## 유사도 계산 알고리즘

유사도를 계산하는 알고리즘은 크게 3가지가 있다.

### `A. 코사인 유사도`

두 벡터의 방향을 기준으로 유사도를 판단한다. 벡터 사이의 각도를 기준으로 측정하므로 값의 범위가 `-1 ~ 1`로 정규화 되어있다.

#### 공식 : cosine_similarity(A,B)||A|| ||B|| A⋅B​=∑Ai2​​⋅∑Bi2​​∑Ai​Bi​​

값의 의미는 다음과 같다.
| 값 | 의미 | 예시 |
| --------------- | ---------------------- | ------------------------------------ |
| **1.0** | 완전히 같은 방향 (100% 동일 의미) | `"나는 커피가 좋아"` ≈ `"커피가 너무 좋아"` |
| **0.8 \~ 1.0** | 매우 유사함 | `"나는 커피를 자주 마셔"` ≈ `"아메리카노를 즐겨 마신다"` |
| **0.5 \~ 0.8** | 꽤 유사함 (부분 겹침) | `"나는 커피가 좋아"` ≈ `"나는 음료를 좋아해"` |
| **0.0** | 무관한 내용 (직각, 방향 없음) | `"나는 커피가 좋아"` ⊥ `"고양이는 귀엽다"` |
| **-1.0 \~ 0.0** | 완전히 반대 방향 (대립 의미) | `"나는 커피가 좋아"` ↔ `"커피는 역겨워"` |

### `B. 유클리드 거리`

두 벡터 사이의 직선 거리를 기준으로 유사도를 판단한다.
값의 범위는 0 ~ ∞ 이다.

#### 공식 : L2_distance(A,B)=∑(Ai​−Bi​)2​

값의 의미는 다음 표와 같다.
| 값 | 의미 | 예시 |
| ------------------------- | -------------------- | ------------------------------ |
| **0.0** | 완전 동일 (벡터가 똑같음) | `"커피는 맛있다"` = `"커피는 맛있다"` |
| **작은 수 (ex. 0.1 \~ 2.0)** | 매우 유사 (벡터 위치가 거의 동일) | `"나는 커피를 좋아해"` ≈ `"커피는 내 최애야"` |
| **중간 수 (ex. 5 \~ 10)** | 적당히 관련 있음 | `"나는 커피가 좋아"` ≈ `"나는 차를 마셔"` |
| **큰 수 (ex. 20 이상)** | 의미가 매우 다름 | `"나는 커피가 좋아"` ↔ `"자동차는 빠르다"` |

### `C. 내적`

두 벡터를 내적해서 나온 값으로 판단한다. 계산은 코사인 유사도와 비슷하지만, 벡터의 크기까지 고려한다는 점에서 값의 범위가 -∞ ~ ∞ 라는 차이가 있다.

## 비교 (각각을 언제 사용할까?)

우선 의미 벡터 공간에서 `크기`와 `방향`이 뭘 의미하는지 이해할 필요가 있다.

#### 방향

벡터의 방향은 그 벡터가 어떤 개념, 주제, 의미를 향하고 있는지를 나타낸다. 예를들어서 '커피'와 '아메리카노'는 유사한 방향을 갖지만, '고양이'는 전혀 다른 방향을 갖는다.

#### 크기

보통 해석이 하나로 고정되지는 않지만, 크게 4가지로 해석할 수 있다.
| 해석 방식 | 설명 |
| ----------------- | ------------------------------------------ |
| **표현 강도** | 해당 의미가 얼마나 **명확하게 드러났는지** (ex. 강한 감정 표현) |
| **확신도** | 모델이 얼마나 **자신 있게** 특정 의미로 임베딩했는가 (신뢰도) |
| **빈도 기반 임베딩일 경우** | 자주 등장한 단어는 **벡터 크기가 작고**, 희귀 단어는 **크기가 크다** |
| **정보량** | 뚜렷한 의미를 가진 벡터일수록 크기가 커지는 경향도 있다 |

#### 거리

유사한 개념, 의미를 갖는 데이터들을 서로 가까이에 붙어있고, 그런지 않은 데이터들은 멀리 떨어져 있다.

위를 기반으로 해서 각각의 유사도 계산 알고리즘의 차이를 이해해보자.

### `코사인 유사도`

해당 단어의 `의미`를 중점으로 판단한다.
| 사례 | 이유 |
| -------------------- | ----------------------- |
| "질문 답변 검색 시스템 (RAG)" | 유사 문장 검색 → 의미 방향만 중요 |
| "문서 추천 시스템" | 유사 주제 문서 추천 |
| "챗봇 기억 검색" | 과거 질문과 현재 질문이 의미상 비슷한가? |

### `유클리드 거리 L2`

벡터 간의 실제 좌표상의 거리가 중요한경우 사용한다. 이미지, 좌표 등에서 비슷한 모양이나 수치를 찾아내는데에 사용된다.

| 사례                              | 이유                    |
| --------------------------------- | ----------------------- |
| "비슷한 손글씨 이미지 찾기"       | 실제 픽셀 간 차이 판단  |
| "좌표 기반 근접 검색 (e.g., GPS)" | 실질적 거리 자체가 중요 |
| "유사한 음성 파형 찾기"           | 수치 기반 차이          |

### `내적`

방향과 크기 모두 고려해야할 때 사용한다. 개개인에 특화되어있는 데이터들을 찾아낼 때 사용한다.

| 사례                 | 이유                                   |
| -------------------- | -------------------------------------- |
| "개인화 추천 시스템" | 유저 선호 벡터와 아이템 특성 벡터 내적 |
| "광고 점수 계산"     | 클릭 확률 예측값 포함                  |
| "랭킹 정렬 시스템"   | 내적값이 높을수록 우선순위 상단에 배치 |

이부분에서 개인의 취향이랑 내적의 값이랑 연관이 있다는 사실이 이해가 안될 수도 있다. `방향성`과 `크기`를 중심으로 다음 예시를 통해 이해해보자.

> "나는 '투썸 녹차 라떼'를 좋아해"

`투썸 말차 라떼`를 좋아하므로, `나의 취향 벡터`가 다음 벡터값을 갖는다. [1, 0.9, 0.9] (투썸, 말차, 라떼)

**방향성** : 말차, 라떼 를 좋아하므로 이와 유사한 방향인 `아몬드 라떼`[투썸=1.0, 아몬드=0.8, 라떼=1.0] 에 대한 선호도 높다는 것을 예측할 수 있다.

**크기** : 하지만 아몬드 라떼[투썸=1.0, 아몬드=0.8, 라떼=1.0]가 말차 라떼보다 크기가 더 작으므로 말차 라떼에 비해 선호가 낮다는 것을 예측할 수 있다.

# 5. 인덱싱

벡터가 많아질수록 계산량이 많아지기 때문에 최소한 적게 비교하여 찾아내야한다.
이를 위해서 `인덱싱`이 필요한데, 2가지 인덱싱을 방법을 알아보자.

## 1. HNSW (Hierarchical Navigable Small World)

벡터들로 그래프를 만들어 둔다. 그리고 이 그래프를 기반으로 좁혀 들어가는 방식이다. Tree와 유사하게 상위 노드에서 하위 노드로 타고 들어가면서 `벡터 거리`를 기준으로 유사도를 판단하여 깊게 들어간다. 내려가는 과정에서 관련없는 데이터들이 필터링 되고, 마지막에는 찾고자 하는 데이터들만 찾아낼 수 있다.

출처 : https://www.researchgate.net/figure/An-illustration-of-an-HNSW-Graph_fig1_334028001

### A. 그래프 생성 방식

1. 새로운 벡터가 들어오면, 현재 존재하는 벡터들 중 일부와 거리 기반으로 연결시킨다.
2. 연결한 이웃의 개수는 최 근방 M개로 제한
3. 일부 벡터는 더 높은 레벨의 그래프에도 배치되어서, 필터링 기준 역할을 한다.

### B. 탐색 방식

1. 최상단 레벨의 랜덤 노드에서 탐색을 시작한다.
2. 거리상으로 가장 가까운 이웃으로 점프하면서 거리차이를 줄인다.
3. 거리가 가장 가까운 노드인 경우, 하위레벨로 내려간다.
4. 2-3을 반복
5. 최하단 레벨에서 거리가 가까운 벡터 리스트를 탑색한다.

## 2. IVF (Inverted File Index)

IVF 방식은 크게 두 과정으로 나뉘어진다.

1. 인덱스를 생성하는 과정
2. 검색하는 과정

### 인덱스 생성

1. KMeans 알고리즘을 이용하여 입력된 데이터들을 k개의 그룹으로 나눈다.
2. 각 클러스터별로 내부 중심점과 클러스터에 속한 vector 들 정보를 담는 bucket 을 만든다.

### 검색

1. 검색하고 싶은 벡터를 넣는다.
2. 검색할 벡터와 각 클러스터 중심점 사이의 유사도를 계산한다. 상위 N개만 뽑는다.
3. 선택한 bucket N개에 속한 벡터들와 유사도를 계산한다.
4. 유사도를 기준으로 정렬하여 상위 K개만 추출한다.

# Word2Vec

## Word2Vec이란?

단어를 벡터로 바꿔주는 모델이다. 비슷한 의미의 단어들은 비슷한 벡터로 변환해준다.

## 학습 방식

### CBOW

주변 단어들을 통해서 중심 단어를 예측하는 방식이다. 예시를 들자면 "나는", "먹었다" 에서 "밥"을 예측하는 방식이다.

### Skip-gram

중심 단어들을 통해서 주변 단어들을 예측한다. 예를 들어 "밥"을 입력으로 준다면 "나는"과 "먹었다"를 예측하여 준다.

### 학습은 아래 과정을 거친다.

1. 텍스트 데이터를 넣어준다
2. 단어별로 정수값으로 인덱싱을 진행한다.
3. 윈도우 크기를 설정하여, 해당 단어의 좌우 범위에 영향 정도를 지정한다.
4. 입력과 출력 쌍을 만든다. 위에서 CBOW, Skip-gram 방식을 선정하는 것
5. 신경망을 학습한다.

위에서 학습한 결과의 단어들은 학습 결과는 N차원 벡터로 표현된다.

# 벡터 DB

## 자주 사용되는 벡터 DB

자주 사용되는 벡터 DB는 크게 다음 6개가 있다.
| 이름 | 저장 방식 | 검색 방식 | 특징 |
| ------------ | ----------------------------- | ------------------------------------------------------------- | -------------------------------------------- |
| **FAISS** | 로컬 메모리 (in-memory, 디스크 저장 가능) | 정확도-속도 조절 가능한 Approximate Nearest Neighbor (ANN, IVF, HNSW 등) | 가장 유명한 오픈소스 벡터 라이브러리 (Facebook), Python에 최적화 |
| **Pinecone** | 클라우드 (SaaS) | ANN (HNSW 기반) | 사용 간편함, 서버 관리 필요 없음, 무료 요금제 있음 |
| **Weaviate** | 클라우드 / 로컬 둘 다 가능 | ANN (HNSW) + Graph | RESTful API 제공, 메타데이터 검색 강력 |
| **Milvus** | 디스크 기반 + 클러스터링 지원 | IVF_FLAT, HNSW, PQ 등 | GPU 지원, 초대용량 데이터 처리 가능 |
| **Qdrant** | 디스크 기반 (Rust 기반) | HNSW (빠르고 정확함) | PostgreSQL 같이 쓰기 좋음, JSON metadata 필터링 강력 |
| **Chroma** | 로컬 또는 경량 디스크 저장 | Cosine / L2 distance | LangChain 공식 지원, 빠르게 실험하기 좋음 |

여기에서 저장 방식과 검색 방식에 따라 성능의 차이가 발생하므로 유의깊게 볼 필요가 있다.
